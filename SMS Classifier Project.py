#!/usr/bin/env python
# coding: utf-8

# # Company : Bharat Intern

# ***Position : Data Science Intern***
# 

# ***Auther : Mosheer Khan***

# ***Project Description : SMS Classifier Project***

#  Develop a text classification model to classify SMS as either **spam or non-spam** using data science techiques in python

# ***Stages to perform this project:***

# ***1.Data Gathering***

# ***2.Data Cleaning***

# ***3.Exploratory Data Analysis (EDA)***

# ***4.Text preprocessing***

# ***5.Model Building***

# ***6.Evaluation of Model***

# ***7.Website***

# # **Step- 1 DATA GATHERING**

# In[5]:


import numpy as np


# In[6]:


import pandas as pd


# In[15]:


import seaborn as sns


# In[19]:


import matplotlib.pyplot as plt


# In[26]:


df=pd.read_csv(r"C:\Users\Mosheer\Downloads\spam.csv",encoding='latin1')
df


# In[27]:


#Check the row and columns:
df.shape


# # Step- 2 DATA CLEANING

# In[28]:


#Check the cloumns name and their datatype:
df.info


# In[31]:


#Drop the unncessary columns i.e last 3 columns:
df.drop(columns=['Unnamed: 2' , 'Unnamed: 3' , 'Unnamed: 4'],inplace = True)


# In[32]:


df.head()


# In[33]:


#Rename the columns name
df.rename(columns={'v1' : 'Target' , 'v2' : 'Text'} , inplace=True)


# In[34]:


df.head()


# In[37]:


#Applying label Encoder for the Target column:
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()


# In[39]:


df['Target'] = encoder.fit_transform(df['Target'])


# In[40]:


df.head()


# In[41]:


df.isnull().sum()

#There is no missing values:


# In[43]:


#Check the duplicate values:
df.duplicated().sum()

#There are 403 duplicate values


# In[44]:


#Remove the all duplicate values:
df = df.drop_duplicates(keep = "first")


# In[45]:


#Recheck the column after drop the duplicate column:
df.duplicated().sum()


#There is no any nulls values


# # Step- 3 EXPLORATORY DATA ANALYSIS (EDA)

# In[47]:


df.head()


# In[51]:


#Check how many message are spam or ham:
df['Target'].value_counts()


# In[63]:


#Represent the no. of messages which are spam or ham using MATPLOTLIB
plt.pie(df['Target'].value_counts(), colors = [ 'Green', 'red'], labels=['ham','spam'], autopct="%0.1f")
plt.title('Distribution of ham and spam in percentage')

plt.show()


# # Step- 4 TEXT PREPROCESSING USING NLTK LIBRARY

# ***To summarize the task using the NLTK library***

# 1.Counting character: Use NLTK to count the no. of alphabets in the SMS text.

# 2.Counting words: Utilize NLTK to count the no. of words in the SMS text.

# 3.Counting sentance: Employ NLTK to count the no. of sentences in the SMS text.

# AND DO ANALYSIS

# In[64]:


#NLTK - Natural Language Toolkit
get_ipython().system('pip install nltk')


# In[66]:


get_ipython().system('pip install nltk')


# In[67]:


import nltk


# In[68]:


import nltk
nltk.download('punkt')


# # Check Number of Characters:

# In[70]:


df['num_characters'] = df['Text'].apply(len)
df.head()


# In[71]:


# Use of nltk library
df['num_words'] = df['Text'].apply(nltk.word_tokenize)
df


# In[72]:


# Count number of words in each SMS text
df['num_words'] = df['num_words'].apply(len)
df


# # Check Number of Sentences:

# In[73]:


df['num_sentences'] = df['Text'].apply(nltk.sent_tokenize)
df


# In[74]:


# Count number of sentence in each sms text

df['num_sentences'] = df['num_sentences'].apply(len)
df.head()


# In[75]:


# See the descriptive statistics data:
df[['num_characters', 'num_words', 'num_sentences']].describe()


# In[77]:


# See ham messages:
df.loc[df['Target'] == 0][['num_characters', 'num_words', 'num_sentences']].describe()


# In[78]:


# See spam messages:
df.loc[df['Target'] == 1][['num_characters', 'num_words', 'num_sentences']].describe()


# # Note:

# The Descriptive Statistics of ham and spam messages indicate that, on average, spam messages tend to be slightly larger in size compared to ham messages.

# # Characters:

# In[82]:


# See the histplot of ham messages and spam messages:
plt.figure(figsize=(12,5))
ham_messages = df.loc[df['Target']==0][['num_characters']]            # ham messages
spam_messages = df.loc[df['Target']==1][['num_characters']]           # spam messages
sns.histplot(spam_messages['num_characters'], color='Red')
sns.histplot(ham_messages['num_characters'], color='yellow')
plt.title('Distribution of ham messages and spam messages character')
plt.show()


# In[85]:


# See the histplot of ham messages and spam messages:
plt.figure(figsize=(12,5))
ham_messages = df.loc[df['Target']==0][['num_characters']]            # ham messages
spam_messages = df.loc[df['Target']==1][['num_characters']]           # spam messages
sns.histplot(spam_messages['num_characters'], color='Red')
sns.histplot(ham_messages['num_characters'], color='Yellow')
plt.title('Distribution of ham messages and spam messages words')
plt.show()


# # Finding relation between Number of characters and Number of words:

# In[86]:


sns.pairplot(df, hue='Target')
plt.show()


# By seeing the above pairplot we can say that there are some outlier in the dataset.

# # Correlation Coefficient

# In[87]:


df[['Target', 'num_characters', 'num_words', 'num_sentences']].corr()


# In[90]:


## Use heatmap for better understanding:
sns.heatmap(df[['Target', 'num_characters', 'num_words', 'num_sentences']].corr(), annot=True)    
plt.show()


# NOTE: annot=True in a heatmap function adds numerical annotations to each cell of the heatmap for improved data interpretation.

# # Analyze by the above Heatmap:

# The correlation analysis indicates that there is a positive correlation of ğŸ¬.ğŸ¯ğŸ´ between the number of characters in a message and the likelihood of it being classified as spam. This suggests that as the number of characters in a message increases, there is a tendency for it to be classified as spam, potentially due to certain characteristics or patterns associated with longer messages in the context of spam classification.

# ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡ - The decision to use the "num_characters" column in model creation is driven by its correlation coefficient of ğŸ¬.ğŸ¯ğŸ´ with the target variable, indicating a stronger association compared to "num_words" and "num_sentences."

# # Data Preprocessing:

# The below code is performing data preprocessing on the DataFrame. It includes steps like removing punctuation, converting text to lowercase, splitting text into words, and lemmatizing the words.
# 
# â€£ Lower case - ğ¶ğ‘œğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘¡ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘  ğ‘–ğ‘› ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ ğ‘ğ‘ğ‘ ğ‘’.
# â€£ Tokenization - ğ˜±ğ˜³ğ˜°ğ˜¤ğ˜¦ğ˜´ğ˜´ ğ˜°ğ˜§ ğ˜£ğ˜³ğ˜¦ğ˜¢ğ˜¬ğ˜ªğ˜¯ğ˜¨ ğ˜¥ğ˜°ğ˜¸ğ˜¯ ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜ªğ˜¯ğ˜µğ˜° ğ˜´ğ˜®ğ˜¢ğ˜­ğ˜­ğ˜¦ğ˜³ ğ˜¶ğ˜¯ğ˜ªğ˜µğ˜´â¸´ ğ˜´ğ˜¶ğ˜¤ğ˜© ğ˜¢ğ˜´ ğ˜¸ğ˜°ğ˜³ğ˜¥ğ˜´
# â€£ Removing special characters -
# 
#  Eg: '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'  etc.
#  
# â€£ Removing stopwords and punctuation -
#  Eg: (a, an, the, and, but, or, in, on, at, by, with, from, to, of, for, I, you, he, she, it, we, they, me, him, her,
#        us, them, is, are, am, was, were, be, being, been, have, has, had, do, does, did, will, would, shall, should,
#        can, could, may, might, must) etc.

# â€£ Stemming - Involves ğ™‡ğ™šğ™¢ğ™¢ğ™ğ™¯ğ™–ğ™©ğ™ğ™¤ğ™£
#  To reduce words to their base or root form, ensuring consistency and simplification for analysis, such as converting variations like "dancing," "dance," and "danced" to their common base form "dance" to streamline text processing and analysis.

# In[91]:


from nltk.corpus import stopwords       # ğ™¨ğ™©ğ™¤ğ™¥ğ™¬ğ™¤ğ™§ğ™™ğ™¨ module is used to access a list of common stopwords in various languages
from nltk.stem.porter import PorterStemmer
import string                           # Import string
ps = PorterStemmer()


# In[92]:


def transform_text(text):
    text = text.lower()                 # Converting text to lowercase
    text = nltk.word_tokenize(text)     # Doing tokenize to break down text into small words of list
    
    y = []                    
    for i in text:
        if i.isalnum():                 # Removing special characters
            y.append(i)
              
    text = y[:]                    # Assign text to y                                  
    y.clear()                      # Clear the y
    
    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:     # Removing stopwords and punctuation
            y.append(i)
            
            
    text = y[:]                    # Again, Assign text to y
    y.clear()                      # And clear the y
    
    for i in text:
        y.append(ps.stem(i))
        
    return " ".join(y)             # Joining of list of data into a single string


# In[98]:


transform_text("I loved the YT lectures on Machine Learning. How about you?")


# In[102]:


df['transformed_text'] = df['Text'].apply(transform_text)


# In[103]:


df.head()


# # Word Cloud

# ğğ¨ğ° ğ°ğ ğœğšğ§ ğ°ğ¨ğ«ğ¤ğ¢ğ§ğ  ğ­ğ¨ ğŸğ¢ğ§ğ ğ“ğ¨ğ© ğ°ğ¨ğ«ğğ¬ ğ°ğ¡ğ¢ğœğ¡ ğšğ«ğ ğ®ğ¬ğğ ğ¢ğ§ ğ¡ğšğ¦ ğšğ§ğ ğ¬ğ©ğšğ¦ ğ¦ğğ¬ğ¬ğšğ ğğ¬ ğšğ§ğ ğ®ğ¬ğ ğ°ğ¨ğ«ğ ğœğ¥ğ¨ğ®ğ ğ­ğ¨ ğ¬ğ¡ğ¨ğ° ğ­ğ¡ğ ğ«ğğ©ğ«ğğ¬ğğ§ğ­ğšğ­ğ¢ğ¨ğ§:

# In[105]:


get_ipython().system(' pip install wordCloud')


# # Top words in spam messages:

# In[111]:


plt.figure(figsize=(8,15))
from wordcloud import WordCloud         # Use wordcloud module to show the top text word used in ham and spam messages
wc = WordCloud(width=1000, height=1000, min_font_size=10, background_color='Black')
spam_wc = wc.generate(df[df['Target'] == 1]['transformed_text'].str.cat(sep=" "))    # Use generate function, and 1 for "spam"
plt.imshow(spam_wc) 
plt.show()
# To show the word cloud chart


# In[110]:


plt.figure(figsize=(8, 15))
wc = WordCloud(width=1000, height=1000, min_font_size=10, background_color='white')
ham_wc = wc.generate(df[df['Target']==0]['transformed_text'].str.cat(sep=" "))
plt.imshow(ham_wc) 
plt.show()
# To show the word cloud chart


# # ğ“ğ¨ğ© 40 ğ°ğ¨ğ«ğğ¬ ğ®ğ¬ğ ğ¢ğ§ ğ¬ğ©ğšğ¦ ğ¦ğğ¬ğ¬ğšğ ğ:

# In[112]:


df.loc[df['Target']==1]['transformed_text']


# In[116]:


plt.figure(figsize=(15,5))
from collections import Counter      # Use Counter module
spam_words = []
for message in df.loc[df['Target']==1]['transformed_text'].tolist():
    for words in message.split():
        spam_words.append(words)
        
spam_data = pd.DataFrame(Counter(spam_words).most_common(40), columns=['VALUE', 'COUNT'])    # Most 30 common words
ax = sns.barplot(x='VALUE', y='COUNT', data=spam_data)
plt.title('Top 40 Most common words used in spam messages')
plt.xticks(rotation=90)
for bars in ax.containers:
    ax.bar_label(bars)


# # ğ“ğ¨ğ© 40 ğ°ğ¨ğ«ğğ¬ ğ®ğ¬ğ ğ¢ğ§ ğ¡ğšğ¦ ğ¦ğğ¬ğ¬ğšğ ğ:

# In[119]:


df.loc[df['Target']==0]['transformed_text']


# In[121]:


plt.figure(figsize=(15,5))
from collections import Counter
ham_words = []
for messages in df.loc[df['Target']==0]['transformed_text'].tolist():
    for words in messages.split():
        ham_words.append(words)

ham_data = pd.DataFrame(Counter(ham_words).most_common(40), columns=['VALUE', 'COUNT'])    # Most 30 common words
ax = sns.barplot(x='VALUE', y='COUNT', data=ham_data)
plt.title('Top 40 Most common words used in ham messages')
plt.xticks(rotation=90)
for bars in ax.containers:
    ax.bar_label(bars)


# # Step- 5 Model Building

# ***Naive Bayes Algorithmn***

# 
# For an SMS classifier project, Naive Bayes algorithms is suitable choice due to the nature of textual data and the simplicity of the algorithm.

# ğˆğ§ğ©ğ®ğ­: transformed_text (ğ™ğ™–ğ™¨ğ™  ğ™©ğ™¤ ğ™˜ğ™¤ğ™£ğ™«ğ™šğ™§ğ™© ğ™ğ™£ğ™©ğ™¤ ğ™£ğ™ªğ™¢ğ™šğ™§ğ™ğ™˜ğ™–ğ™¡ ğ™«ğ™–ğ™¡ğ™ªğ™š)
# 
# ğğ®ğ­ğ©ğ®ğ­: target

# ***Ná´á´›á´‡:***
# For building machine learning model the input data should be numerical/ vector, but here 'transformed_text' is in string format so we have deal with it using "bag of words".

# # ğ—–ğ—¼ğ˜‚ğ—»ğ˜ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ğ—¶ğ˜‡ğ—²ğ—¿
# 

# ***ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ›ğšğ  ğ¨ğŸ ğ°ğ¨ğ«ğğ¬ (ğ—–ğ—¼ğ˜‚ğ—»ğ˜ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ğ—¶ğ˜‡ğ—²ğ—¿)?***

# The bag of words model simplifies text into numbers by counting how often words appear, making it easier for computers to analyze and process text data for tasks like classification and analysis.

# In[122]:


# Use of ğ—–ğ—¼ğ˜‚ğ—»ğ˜ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ğ—¶ğ˜‡ğ—²ğ—¿ class from ğ˜€ğ—¸ğ—¹ğ—²ğ—®ğ—¿ğ—».ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²_ğ—²ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—».ğ˜ğ—²ğ˜…ğ˜ module for converting collection of text documents into a
# matrix of token counts.

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer 
cv = CountVectorizer()

X1 = cv.fit_transform(df['transformed_text']).toarray()

print("SMS:", X1.shape[0])
print("Total Words:", X1.shape[1])


# In[123]:


print(X1)


# In[125]:


y1 = df['Target'].values
y1


# # Now we apply train-test split()

# In[126]:


from sklearn.model_selection import train_test_split


# # Why we use?

# Naive Bayes classifiers, such as MultinomialNB and BernoulliNB, are commonly used for SMS classifier projects because they are efficient, easy to implement, effective for text classification tasks, handle sparse data well, scale to large datasets, and achieve good performance in practice.

# ğƒğ¢ğŸğŸğğ«ğğ§ğœğ ğ›ğğ­ğ°ğğğ§ ğ­ğ¡ğğ¬ğ ğğšğ¢ğ²ğ ğğšğ²ğğ¬ ğ€ğ¥ğ ğ¨ğ«ğ¢ğ­ğ¡ğ¦ğ§ğ¬â“
# 
# â€¢ ğ—šğ—®ğ˜‚ğ˜€ğ˜€ğ—¶ğ—®ğ—»ğ—¡ğ—• works with data that looks like a bell curve, ğ—ºğ—²ğ—®ğ—»ğ˜€: it is typically used for features that are continuous or real-valued.
# 
# â€¢ ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ğ—¡ğ—• works with counting occurrences, like words in text.
# 
# â€¢ ğ—•ğ—²ğ—¿ğ—»ğ—¼ğ˜‚ğ—¹ğ—¹ğ—¶ğ—¡ğ—• works with situations where you're just interested in whether something happens or not, like yes or no questions.

# # ğ—ªğ—µğ˜† ğ˜„ğ—² ğ˜‚ğ˜€ğ—² ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—»_ğ˜€ğ—°ğ—¼ğ—¿ğ—²â“

# ğ—¥ğ—˜ğ—”ğ—¦ğ—¢ğ—¡: Precision help to make sure that SMS classifier is accurate in catching spam while avoiding mistakes that could upset users or miss business goals.

# In[128]:


# Import all the Naive Bayes Algorithmn because I don't know the distribution of data.

X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=2)   # 20% data for test
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

gnb1 = GaussianNB()                
mnb1 = MultinomialNB()
bnb1 = BernoulliNB()


# # Gaussian Naive Bayes (GaussianNB):

# In[129]:


gnb1.fit(X1_train, y1_train)                                         # train the model
y1_predict1 = gnb1.predict(X1_test)
print("accuracy score:", accuracy_score(y1_test, y1_predict1))      # The accuracy is 88%
print("confusion_matrix:", confusion_matrix(y1_test, y1_predict1))
print("precision_score:", precision_score(y1_test, y1_predict1))    # The precision score is 53%


# ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡ ğ—§ğ—”ğ—ğ—˜ğ—¡- The ğ—šğ—®ğ˜‚ğ˜€ğ˜€ğ—¶ğ—®ğ—» ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier achieved an ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† of ğŸ–ğŸ–%, indicating reasonable overall performance. However, its ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ¬.ğŸ±ğŸ¯ suggests it struggles to accurately identify spam, potentially leading to a significant number of false positives. Given the importance of precision in spam detection to maintain user satisfaction, I choose not to use the ğ—šğ—®ğ˜‚ğ˜€ğ˜€ğ—¶ğ—®ğ—» ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier.

# # Multinomial Naive Bayes (MultinomialNB):

# In[130]:


mnb1.fit(X1_train, y1_train)
y1_predict2 = mnb1.predict(X1_test)
print("accuracy score:", accuracy_score(y1_test, y1_predict2))      # The accuracy is 96%
print("confusion_matrix:", confusion_matrix(y1_test, y1_predict2))  
print("precision_score:", precision_score(y1_test, y1_predict2))    # The precision score is 83%


# ğ—¡ğ—¼ğ˜ğ—²: I need to focus on ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—»_ğ˜€ğ—°ğ—¼ğ—¿ğ—² first rather than accuracy because the data are imbalanced.
# 
# ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡ ğ—§ğ—”ğ—ğ—˜ğ—¡- The ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier achieved an ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† of ğŸµğŸ²%, indicating reasonable overall performance. However, its ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ¬.ğŸ´ğŸ¯ suggests it struggles to accurately identify spam, potentially leading to a significant number of false positives. Given the importance of precision in spam detection to maintain user satisfaction, I choose not to use the ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier.

# # Bernoulli Naive Bayes (BernoulliNB)

# In[131]:


bnb1.fit(X1_train, y1_train)
y1_predict3 = bnb1.predict(X1_test)
print("accuracy score:", accuracy_score(y1_test, y1_predict3))      # The accuracy is 97%
print("confusion_matrix:", confusion_matrix(y1_test, y1_predict3))
print("precision_score:", precision_score(y1_test, y1_predict3))    # The precision score is 97%


# ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡ ğ—§ğ—”ğ—ğ—˜ğ—¡- The ğ—•ğ—²ğ—¿ğ—»ğ—¼ğ˜‚ğ—¹ğ—¹ğ—¶ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier achieved an ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† of ğŸµğŸ³%, indicating reasonable overall performance and the ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ¬.ğŸµğŸ³ indicates that ğ—•ğ—²ğ—¿ğ—»ğ—¼ğ˜‚ğ—¹ğ—¹ğ—¶ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ is better than rest of the two algorithmn.

# # ğ“ğŸğ¢ğğŸğ•ğğœğ­ğ¨ğ«ğ¢ğ³ğğ«

# What is ğ“ğŸğ¢ğğŸğ•ğğœğ­ğ¨ğ«ğ¢ğ³ğğ«?
# 
# ğ“ğŸğ¢ğğŸğ•ğğœğ­ğ¨ğ«ğ¢ğ³ğğ« is a tool in natural language processing that transforms text data into numbers for machine learning. ğ—œğ˜ ğ—®ğ˜€ğ˜€ğ—¶ğ—´ğ—»ğ˜€ ğ˜€ğ—°ğ—¼ğ—¿ğ—²ğ˜€ ğ˜ğ—¼ ğ˜„ğ—¼ğ—¿ğ—±ğ˜€ ğ—¯ğ—®ğ˜€ğ—²ğ—± ğ—¼ğ—» ğ˜ğ—µğ—²ğ—¶ğ—¿ ğ—¶ğ—ºğ—½ğ—¼ğ—¿ğ˜ğ—®ğ—»ğ—°ğ—² ğ—¶ğ—» ğ—²ğ—®ğ—°ğ—µ ğ—±ğ—¼ğ—°ğ˜‚ğ—ºğ—²ğ—»ğ˜ ğ—®ğ—»ğ—± ğ—®ğ—°ğ—¿ğ—¼ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—²ğ—»ğ˜ğ—¶ğ—¿ğ—² ğ—±ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜. This helps algorithms understand the context and significance of words. In short, TfidfVectorizer makes text data understandable for machine learning algorithms by giving numerical importance to words.

# In[132]:


# Use of ğ“ğŸğ¢ğğŸğ•ğğœğ­ğ¨ğ«ğ¢ğ³ğğ« class from ğ˜€ğ—¸ğ—¹ğ—²ğ—®ğ—¿ğ—».ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²_ğ—²ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—».ğ˜ğ—²ğ˜…ğ˜ 

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=3000)                     # Use max 3000 words

X = tfidf.fit_transform(df['transformed_text']).toarray()

print(X.shape)


# In[134]:


y = df['Target'].values
y


# # Now we apply train-test split()

# In[135]:


from sklearn.model_selection import train_test_split


# In[136]:


# Import all the Naive Bayes Algorithmn because I don't know the distribution of data.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)   # 20% data for test
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score


# Why we use precision_score?
# Rá´‡á´€sá´É´: Precision help to make sure that SMS classifier is accurate in catching spam while avoiding mistakes that could 
# upset users or miss business goals.


gnb = GaussianNB()                
mnb = MultinomialNB()
bnb = BernoulliNB()


# # Gaussian Naive Bayes (GaussianNB):

# In[137]:


gnb.fit(X_train, y_train)                                         # train the model
y_pred1 = gnb.predict(X_test)
print("accuracy score:", accuracy_score(y_test, y_pred1))      # The accuracy is 86%
print("confusion_matrix:", confusion_matrix(y_test, y_pred1))
print("precision_score:", precision_score(y_test, y_pred1))    # The precision score is 50%


# ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡ ğ—§ğ—”ğ—ğ—˜ğ—¡- The ğ—šğ—®ğ˜‚ğ˜€ğ˜€ğ—¶ğ—®ğ—» ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier achieved an ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† of ğŸ´ğŸ³%, indicating reasonable overall performance. However, its ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ¬.ğŸ±ğŸ¬ suggests it struggles to accurately identify spam, potentially leading to a significant number of false positives. Given the importance of precision in spam detection to maintain user satisfaction, I choose not to use the ğ—šğ—®ğ˜‚ğ˜€ğ˜€ğ—¶ğ—®ğ—» ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier.

# # Multinomial Naive Bayes (MultinomialNB):

# In[138]:


mnb.fit(X_train, y_train)
y_pred2 = mnb.predict(X_test)
print("accuracy score:", accuracy_score(y_test, y_pred2))      # The accuracy is 97%
print("confusion_matrix:", confusion_matrix(y_test, y_pred2))  
print("precision_score:", precision_score(y_test, y_pred2))    # The precision score is 1.0%


# ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡ ğ—§ğ—”ğ—ğ—˜ğ—¡- The ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier achieved an ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† of 97%, indicating reasonable overall performance and ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ­.ğŸ¬ which means that ğ˜ğ—µğ—¶ğ˜€ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—±ğ—¼ğ—»âœğ˜ ğ—´ğ—¶ğ˜ƒğ—² ğ—®ğ—»ğ˜† ğ—™ğ—®ğ—¹ğ˜€ğ—² ğ—½ğ—¼ğ˜€ğ—¶ğ˜ğ—¶ğ˜ƒğ—² and it is suitable also.

# # Bernoulli Naive Bayes (BernoulliNB)

# In[139]:


bnb.fit(X_train, y_train)
y_pred3 = bnb.predict(X_test)
print("accuracy score:", accuracy_score(y_test, y_pred3))      # The accuracy is 98%
print("confusion_matrix:", confusion_matrix(y_test, y_pred3))
print("precision_score:", precision_score(y_test, y_pred3))    # The precision score is 99%


# ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡ ğ—§ğ—”ğ—ğ—˜ğ—¡- The ğ—•ğ—²ğ—¿ğ—»ğ—¼ğ˜‚ğ—¹ğ—¹ğ—¶ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ classifier achieved an ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† of 98%, indicating reasonable overall performance and the ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ¬.ğŸµğŸµ indicates that ğ—•ğ—²ğ—¿ğ—»ğ—¼ğ˜‚ğ—¹ğ—¹ğ—¶ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ is better than rest of the two algorithmn.

# # ğ—¥ğ—œğ—šğ—›ğ—§ ğ—¡ğ—¢ğ—ª ğ——ğ—˜ğ—–ğ—œğ—¦ğ—œğ—¢ğ—¡

# The ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ (ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ğ—¡ğ—•) with ğ“ğŸğ¢ğğŸğ•ğğœğ­ğ¨ğ«ğ¢ğ³ğğ« are best fit for the ğ—¦ğ— ğ—¦-ğ—–ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—²ğ—¿ ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ due to its ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ­.ğŸ¬ and ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† ğ—¶ğ˜€ ğŸµğŸ±%, indicating that ğ˜ğ—µğ—¶ğ˜€ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—±ğ—¼ğ—»âœğ˜ ğ—´ğ—¶ğ˜ƒğ—² ğ—®ğ—»ğ˜† ğ—™ğ—®ğ—¹ğ˜€ğ—² ğ—£ğ—¼ğ˜€ğ—¶ğ˜ğ—¶ğ˜ƒğ—².
# 
# However, I'm exploring other algorithms to determine if any of them might be suitable for the ğ—¦ğ— ğ—¦ ğ—–ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—²ğ—¿ ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜.
# 

# # ğ—˜ğ—«ğ—£ğ—Ÿğ—¢ğ—¥ğ—˜:
# # ğ‹ğ¨ğ ğ¢ğ¬ğ­ğ¢ğœğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§
# # ğƒğğœğ¢ğ¬ğ¢ğ¨ğ§ğ“ğ«ğğğ‚ğ¥ğšğ¬ğ¬ğ¢ğŸğ¢ğğ«
# # ğŠğğğ¢ğ ğ¡ğ›ğ¨ğ«ğ¬ğ‚ğ¥ğšğ¬ğ¬ğ¢ğŸğ¢ğğ«
# # ğ‘ğšğ§ğğ¨ğ¦ğ…ğ¨ğ«ğğ¬ğ­ğ‚ğ¥ğšğ¬ğ¬ğ¢ğŸğ¢ğğ«

# In[140]:


from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier


# In[141]:


# Creating object:
lrc = LogisticRegression(solver='liblinear', penalty='l1')
dtc = DecisionTreeClassifier(max_depth=5)
knc = KNeighborsClassifier()
rfc = RandomForestClassifier(n_estimators=50, random_state=2)


# In[142]:


# Creating a dictionary to hold the classifier:
# keys   -> Algorithmn name
# values -> object name
classifiers = {
    'LR' : lrc,
    'DT' : dtc,
    'KN' : knc,
    'RF' : rfc
}


# In[143]:


def train_classifier(classifier, X_train, y_train, X_test, y_test):
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    return accuracy, precision 


# In[144]:


train_classifier(lrc, X_train, y_train, X_test, y_test)


# In[145]:


accuracy_scores = []
precision_scores = []

for name, classifier in classifiers.items():
    current_accuracy, current_precision = train_classifier(classifier, X_train, y_train, X_test, y_test)
    
    print("For:", name)
    print("Accuracy:", current_accuracy)
    print("Precision:", current_precision)
    
    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)
    


# In[146]:


performance_df = pd.DataFrame({'algorithm' : classifiers.keys(), 'accuracy': accuracy_scores, 'Precision': precision_scores})
performance_df.sort_values('Precision', ascending=False)


# # ğ…ğˆğğ€ğ‹ ğƒğ„ğ‚ğˆğ’ğˆğğ

# ğ—•ğ˜† ğ—²ğ˜…ğ—®ğ—ºğ—¶ğ—»ğ—² ğ—®ğ—»ğ—± ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—½ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² ğ—¼ğ—³ ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ˜ ğ—®ğ—¹ğ—´ğ—¼ğ—¿ğ—¶ğ˜ğ—µğ—ºğ˜€
# 
# I can say that only the ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€ (ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ğ—¡ğ—•) with ğ“ğŸğ¢ğğŸğ•ğğœğ­ğ¨ğ«ğ¢ğ³ğğ« have ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ­.ğŸ¬ and ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† ğ—¶ğ˜€ ğŸµğŸ±% which is best for ğ—¦ğ— ğ—¦-ğ—–ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—²ğ—¿ ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜, while ğ—ğ—¡ğ—²ğ—®ğ—¿ğ—²ğ˜€ğ˜ ğ—¡ğ—²ğ—¶ğ—´ğ—µğ—¯ğ—¼ğ—¿ (ğ—ğ—¡ğ—¡) ğ—°ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—²ğ—¿ also achieves a ğ—½ğ—¿ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ˜€ğ—°ğ—¼ğ—¿ğ—² of ğŸ­.ğŸ¬, but its ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜† ğ—³ğ—®ğ—¹ğ—¹ğ˜€ ğ˜€ğ—µğ—¼ğ—¿ğ˜ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—²ğ—± ğ˜ğ—¼ ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—¼ğ—ºğ—¶ğ—®ğ—¹ğ—¡ğ—• ğ˜„ğ—¶ğ˜ğ—µ ğ—§ğ—³ğ—¶ğ—±ğ—³ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ğ—¶ğ˜‡ğ—²ğ—¿, rendering it less suitable for ğ—¦ğ— ğ—¦-ğ—–ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—²ğ—¿ ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜.

# # PICKLE

# Pickle in Python is primarily used in serialzing and deserializing a Python Object structure. In other words, it's the process of converting a Python object into a byte stream (0,1) to store it in a file/database, maintain program state acrros session, or transport data over the network.
# 
# 
# ğ—¦ğ—²ğ—¿ğ—¶ğ—®ğ—¹ğ—¶ğ˜‡ğ—¶ğ—»ğ—´: Converting a Python object into a byte stream. (ğ—£ğ—¶ğ—°ğ—¸ğ—¹ğ—²)

# In[147]:


import pickle  # Import the pickle module for serialization

# Serialize and save the 'tfidf' object to a file named 'vectorizer.pkl'
# 'wb'(write binary) mode is used for writing the file in binary mode
pickle.dump(tfidf, open('vectorizer.pkl', 'wb'))  # Serialize tfidf and save to 'vectorizer.pkl'

# Serialize and save the 'mnb' object to a file named 'model.pkl'
# 'wb' mode is used for writing the file in binary mode
pickle.dump(mnb, open('model.pkl', 'wb'))  # Serialize mnb and save to 'model.pkl'


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:




